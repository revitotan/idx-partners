# -*- coding: utf-8 -*-
"""Final Task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10H9VQjSKabr9umavPxIiSmz8F2hfYTCZ

## Import Packages
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""## Read the Data"""

df = pd.read_csv('loan_data_2007_2014.csv', index_col=0)

df.shape

"""## Exploratory Data Analysis"""

df.head()

df.info()

df.describe().transpose()

nan_df_percent = df.isnull().sum() / df.shape[0] * 100

sns.barplot(x=nan_df_percent[nan_df_percent > 1].sort_values().index, y=nan_df_percent[nan_df_percent > 1].sort_values().values)
plt.xticks(rotation=90)
plt.show()

"""### Delete columns in high missing data columns"""

nan_df_percent[nan_df_percent > 40]

# Drop columns with many missing values (above 40%)
df.drop(nan_df_percent[nan_df_percent > 40].index, axis=1, inplace=True)

nan_df_percent = df.isnull().sum() / df.shape[0] * 100

"""### Delete rows in small missing data columns"""

nan_df_small_percent = nan_df_percent[(nan_df_percent > 0) & (nan_df_percent < 1)].sort_values(ascending=False)
nan_df_small_percent

def del_missing_row(df, list_col):
    for col in list_col.index:
        df.drop(df[df[col].isnull()].index, axis=0, inplace=True)
    nan_df_percent = df.isnull().sum() / df.shape[0] * 100
    nan_df_percent = nan_df_percent[nan_df_percent > 0]
    return nan_df_percent

"""### Impute data with mean or mode for the rest of missing data columns"""

nan_df_percent = del_missing_row(df, nan_df_small_percent)
nan_df_percent

df[nan_df_percent.index].info()

df[nan_df_percent.index].describe()

def impute_missing(df, cat_col, num_col):
    for col in cat_col.index:
        df[col].fillna(df[col].mode()[0], inplace=True)
    for col in num_col.index:
        df[col].fillna(df[col].median(skipna=True), inplace=True)
    nan_df_percent = df.isnull().sum() / df.shape[0] * 100
    nan_df_percent = nan_df_percent[nan_df_percent > 0]
    return nan_df_percent

nan_df_percent = impute_missing(df, nan_df_percent[:2], nan_df_percent[2:])
nan_df_percent

df.isnull().sum().sum()

"""### Check the duplicates"""

df.duplicated().sum()

"""### Make the target columns"""

df['loan_status'].value_counts()

# The risk of current status is unknown, so we won't need that
df.drop(df[['loan_status']][df['loan_status'] == 'Current'].index, axis=0, inplace=True)

potential_val = ['Charged Off', 'Default', 'Does not meet the credit policy. Status:Charged Off', 'Late (31-120 days)']
df['Target'] = np.where(df['loan_status'].isin(potential_val), 1, 0)

df['Target'].value_counts(normalize=True)

df.drop(['loan_status'], axis=1, inplace=True)

df.describe(include='object').transpose()

freq_unique_percent = df.describe(include='object').transpose()['freq'] / df.shape[0] * 100
freq_unique_percent

freq_unique_percent = freq_unique_percent[(freq_unique_percent > 10) & (freq_unique_percent < 90)]
freq_unique_percent

num_cols = df.select_dtypes(include='number').columns

df[num_cols].hist(bins=25,
        figsize=(15,25),
        layout=(-1,5),
        edgecolor="black",
        grid=False)

plt.tight_layout()

corr = df[num_cols].corr()['Target'].abs().sort_values(ascending=False)[1:]
corr

"""## Feature Engineering

### Feature Transformation
"""

freq_unique_percent

df['last_credit_pull_d_Jan-16'] = np.where(df['last_credit_pull_d'].isin(['Jan-16']), 1, 0)

df.drop(['last_credit_pull_d'], axis=1, inplace=True)

"""### Feature Selection"""

# For simple model, select a numerical feature with correlation above 10%
corr[corr > 0.1]

selected_cat = ['term', 'grade']

num_feature = df[corr[corr > 0.1].index]
cat_feature = df[['term', 'grade', 'emp_length', 'home_ownership', 'verification_status', 'initial_list_status', 'last_credit_pull_d_Jan-16']]

X = pd.concat([num_feature, cat_feature], axis=1)

X = pd.get_dummies(X, drop_first=True)
y = df['Target']

"""## Model Building"""

from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV

X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.30, random_state=101)

X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.30, random_state=101)

cv_params = {'n_estimators' : [50,100],
              'max_depth' : [10,50],
              'min_samples_leaf' : [0.5,1],
              'min_samples_split' : [0.001, 0.01],
              'max_features' : ["sqrt"],
              'max_samples' : [.5,.9]}

split_index = [0 if x in X_val.index else -1 for x in X_train.index]
custom_split = PredefinedSplit(split_index)

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(random_state=101)

rfc_val = GridSearchCV(rfc, cv_params, cv=5, refit='f1', n_jobs = -1, verbose = 1)

rfc_val.fit(X_train, y_train)

rfc_val.best_params_

"""### Evaluation"""

y_vpred = rfc_val.predict(X_val)

from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay

confusion_matrix(y_val, y_vpred)

disp = ConfusionMatrixDisplay(confusion_matrix(y_val, y_vpred))
disp.plot()

print(classification_report(y_val, y_vpred))

"""### Final Model"""

rfc_opt = RandomForestClassifier(n_estimators = 100, max_depth = 50,
                                min_samples_leaf = 1, min_samples_split = 0.001,
                                max_features="sqrt", max_samples = 0.9, random_state = 101)

rfc_opt.fit(X_temp, y_temp)

"""### Result"""

test_pred = rfc_opt.predict(X_test)

cm = confusion_matrix(y_test, test_pred)

disp = ConfusionMatrixDisplay(cm)
disp.plot()

print(classification_report(y_test, test_pred))